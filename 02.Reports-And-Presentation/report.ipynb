{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "### 1. Dataset Name: **Cora**\n",
    "The **Cora** dataset is one of the most widely used datasets for research in graph-based learning tasks, especially node classification tasks. This dataset consists of scientific publications (papers) in the field of machine learning, represented in the form of a citation network.\n",
    "\n",
    "### 2. Dataset Structure\n",
    "The Cora dataset is structured as a graph where:\n",
    "- **Nodes** represent individual papers.\n",
    "- **Edges** represent citation links between papers. An edge from Node A to Node B indicates that Paper A cites Paper B.\n",
    "\n",
    "### 3. Features\n",
    "Each node (paper) is characterized by a set of features:\n",
    "- **Feature Type**: Bag-of-words representation.\n",
    "- **Feature Vector**: Each paper is represented by a sparse feature vector, where each dimension corresponds to a unique word in the vocabulary of the dataset. \n",
    "- **Dimensionality**: The feature vector for each paper has a high dimensionality due to the large vocabulary size, resulting in a sparse matrix format.\n",
    "\n",
    "### 4. Labels\n",
    "Each node is assigned a label that corresponds to one of several subject categories within the broader machine learning domain. These categories represent the topics or fields of study of the papers and serve as the labels for classification tasks.\n",
    "\n",
    "#### Label Categories\n",
    "The Cora dataset has the following subject categories:\n",
    "- **Machine Learning**\n",
    "- **Data Mining**\n",
    "- **Neural Networks**\n",
    "- **Probabilistic Methods**\n",
    "- **Genetic Algorithms**\n",
    "- **Rule Learning**\n",
    "- **Theory**\n",
    "\n",
    "### 5. Dataset Statistics\n",
    "The Cora dataset provides a balanced structure, making it ideal for training and evaluating models in node classification. Key statistics include:\n",
    "- **Total Nodes (Papers)**: 2,708\n",
    "- **Total Edges (Citations)**: 5,429\n",
    "- **Number of Features**: 1,433 (dimensions in the bag-of-words representation)\n",
    "- **Number of Classes (Subjects)**: 7\n",
    "\n",
    "### 6. Source and Availability\n",
    "The Cora dataset can be sourced from various platforms supporting graph-based machine learning, such as:\n",
    "- **PyTorch Geometric**\n",
    "- **TensorFlow Graphs**\n",
    "\n",
    "These libraries provide preprocessed versions of the dataset, making it convenient for implementing and testing graph neural network models.\n",
    "\n",
    "### 7. Data Usage in Node Classification\n",
    "In this project, the Cora dataset is utilized to train and evaluate a **Graph Neural Network (GNN)** for node classification. Specifically, the model is designed to predict the subject category (label) of each paper based on its citation connections and feature vectors. By leveraging both the structural information (citations) and content information (features), the GNN model aims to accurately classify each paper into its respective subject category.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### 1. Problem Definition\n",
    "The main objective of this project is to classify academic papers in a citation network based on their subject areas, using a **Graph Neural Network (GNN)**. Citation networks are structured as graphs where:\n",
    "- **Nodes** represent individual papers.\n",
    "- **Edges** represent citation relationships between papers.\n",
    "\n",
    "Node classification in citation networks is challenging because it requires understanding both the structure of the network and the features of each node. This project uses a GNN to leverage these relationships and classify each paper according to its subject category.\n",
    "\n",
    "### 2. Model Selection\n",
    "We selected **Graph Convolutional Networks (GCNs)** as our primary model architecture due to their effectiveness in aggregating information from neighbors in a graph structure. GCNs are particularly suitable for node classification tasks, as they combine feature and topological information to produce robust node embeddings.\n",
    "\n",
    "#### Model Architecture\n",
    "The architecture of our GCN model consists of:\n",
    "- **Input Layer**: Receives feature vectors for each node.\n",
    "- **GNN Layers**: Two graph convolutional layers that perform message passing between nodes to aggregate information from neighboring nodes.\n",
    "- **Output Layer**: A softmax layer for multi-class classification, outputting a probability distribution over the possible classes for each node.\n",
    "\n",
    "### 3. Data Preprocessing\n",
    "Before training, we preprocess the data to make it suitable for the GNN:\n",
    "- **Data Splitting**: The Cora dataset is divided into training, validation, and test sets using a standard split provided by PyTorch Geometric.\n",
    "- **Normalization**: Feature vectors are normalized to ensure consistent input values across all nodes.\n",
    "- **Graph Construction**: Edges in the dataset are processed into an adjacency matrix, representing the citation relationships. This matrix is essential for message-passing operations in the GNN layers.\n",
    "\n",
    "### 4. Training Procedure\n",
    "The model is trained using the following setup:\n",
    "- **Loss Function**: Negative log likelihood (NLL) loss, calculated on the output probabilities for the training set.\n",
    "- **Optimizer**: **Adam optimizer** with a learning rate of 0.01 and weight decay of 5e-4 to prevent overfitting.\n",
    "- **Training Loop**:\n",
    "  - For each epoch, the model is set to training mode, and the optimizer’s gradients are zeroed.\n",
    "  - The model performs a forward pass, computes the loss, and backpropagates the error.\n",
    "  - The optimizer updates the model parameters based on the computed gradients.\n",
    "  - **Epochs**: The model is trained for 200 epochs, with loss values tracked over time.\n",
    "\n",
    "### 5. Evaluation Metrics\n",
    "To assess the model's performance, we use the following metrics:\n",
    "- **Accuracy**: The proportion of correctly classified nodes in the test set.\n",
    "- **Precision, Recall, and F1-Score**: Weighted averages for evaluating model performance across all classes, accounting for both relevance and completeness in predictions.\n",
    "\n",
    "These metrics provide a comprehensive view of the model’s performance and allow for comparisons with baseline methods.\n",
    "\n",
    "### 6. Baseline Models for Comparison\n",
    "To understand the effectiveness of the GNN, we compare its performance with baseline models:\n",
    "- **Logistic Regression**: A basic classifier trained only on node features without using graph structure.\n",
    "- **Random Forest**: Another non-graph-based classifier used to evaluate the added value of the GNN’s graph-aware approach.\n",
    "\n",
    "### 7. Visualization of Results\n",
    "Two visualizations are performed to understand the training and embedding behavior of the model:\n",
    "- **Training Curves**: Loss and accuracy curves are plotted over epochs to observe the model's convergence.\n",
    "- **Node Embeddings**: t-SNE or PCA is applied to visualize the final node embeddings, showing how well the model clusters nodes by subject area in a reduced-dimensionality space.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Architecture\n",
    "\n",
    "### 1. Overview of Graph Neural Networks (GNNs)\n",
    "Graph Neural Networks (GNNs) are a class of neural networks designed to operate directly on graph-structured data. They leverage the connections between nodes to propagate information across the graph, enabling the model to learn rich node representations that incorporate both node features and graph structure.\n",
    "\n",
    "For this project, we use a **Graph Convolutional Network (GCN)** as the primary GNN architecture. The GCN operates by performing **message passing** between neighboring nodes, aggregating feature information from each node’s local neighborhood to produce an embedding that captures both the node’s features and its structural context in the graph.\n",
    "\n",
    "### 2. Model Layers and Design\n",
    "\n",
    "The GCN architecture used in this project consists of the following layers:\n",
    "\n",
    "#### **Input Layer**\n",
    "- **Function**: This layer receives the feature vectors for each node in the graph.\n",
    "- **Feature Vector**: Each feature vector is a sparse bag-of-words representation that describes the attributes of each academic paper in the dataset.\n",
    "- **Dimensionality**: The input dimension is equal to the number of features in the dataset (1,433 for the Cora dataset).\n",
    "\n",
    "#### **Graph Convolution Layers**\n",
    "- **Layer 1 (Graph Convolution)**: \n",
    "  - This layer performs the first step of message passing, aggregating feature information from a node’s immediate neighbors.\n",
    "  - The convolution operation is defined by the **GCNConv** function from the PyTorch Geometric library, which applies a weighted sum of neighboring nodes' feature vectors to each node.\n",
    "  - **Activation Function**: A ReLU activation function is applied after the first convolution layer to introduce non-linearity and enable the network to learn complex patterns.\n",
    "  - **Output Dimensionality**: The output of this layer has a dimensionality of 16, defined by the hidden layer size.\n",
    "\n",
    "- **Layer 2 (Graph Convolution)**:\n",
    "  - The second graph convolution layer performs additional message passing to further refine node embeddings based on extended neighborhoods.\n",
    "  - This layer aggregates information from the embeddings generated by the first layer, allowing the model to capture more complex interactions between nodes.\n",
    "  - **Output Dimensionality**: The output of this layer has a dimensionality equal to the number of classes (7 for the Cora dataset), producing a logit score for each class.\n",
    "\n",
    "#### **Output Layer**\n",
    "- **Softmax Activation**: A softmax activation function is applied to the output of the final layer, converting the logits into a probability distribution over the possible classes for each node.\n",
    "- **Multi-Class Classification**: The output represents the model’s prediction for each node’s class, with the highest probability indicating the predicted category.\n",
    "\n",
    "### 3. Message Passing Mechanism\n",
    "The GCN layers rely on **message passing** to propagate information across the graph. In each GCN layer, the following steps occur:\n",
    "1. **Aggregation**: Each node collects feature information from its immediate neighbors.\n",
    "2. **Transformation**: The aggregated information is passed through a learnable weight matrix, allowing the model to learn which features are important for classification.\n",
    "3. **Update**: Each node updates its embedding based on the transformed and aggregated information.\n",
    "\n",
    "Through this process, the model learns an embedding for each node that captures both its own features and the contextual information provided by its neighbors.\n",
    "\n",
    "### 4. Training Process\n",
    "- **Loss Function**: The model uses **negative log likelihood (NLL) loss** on the softmax output to calculate the difference between predicted probabilities and actual class labels for the training nodes.\n",
    "- **Optimization**: The **Adam optimizer** with a learning rate of 0.01 and weight decay of 5e-4 is employed to minimize the loss function.\n",
    "- **Epochs**: The model is trained over 200 epochs, iteratively updating weights to refine the learned embeddings for accurate node classification.\n",
    "\n",
    "### 5. Architectural Advantages\n",
    "The GCN architecture is particularly suitable for citation networks and similar graph-based data because:\n",
    "- It effectively captures **local neighborhood information** while respecting the graph's structure.\n",
    "- It enables the model to learn both node-level and structure-level patterns, which are crucial for tasks like node classification.\n",
    "- By aggregating information across multiple GCN layers, the model captures both **first-order** (direct neighbors) and **higher-order** (neighbors of neighbors) dependencies, making it robust for complex graph structures.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Loading dataset\n",
    "dataset = Planetoid(root='./01.Dataset-And-Codes/data', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Defining the model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Training and testing functions\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(data)\n",
    "        test_mask = data.test_mask\n",
    "        pred = logits[test_mask].max(1)[1]\n",
    "        acc = accuracy_score(data.y[test_mask].cpu(), pred.cpu())\n",
    "        prec = precision_score(data.y[test_mask].cpu(), pred.cpu(), average='weighted')\n",
    "        rec = recall_score(data.y[test_mask].cpu(), pred.cpu(), average='weighted')\n",
    "        f1 = f1_score(data.y[test_mask].cpu(), pred.cpu(), average='weighted')\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "# Function to visualize node embeddings using t-SNE or PCA\n",
    "def visualize_embeddings(embeddings, labels, method='pca'):\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=2)\n",
    "    elif method == 'tsne':\n",
    "        reducer = TSNE(n_components=2)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'tsne'\")\n",
    "\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "    plt.colorbar(scatter, label=\"Classes\")\n",
    "    plt.title(f'Node Embeddings Visualization using {method.upper()}')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.show()\n",
    "\n",
    "# Tracking loss and accuracy over epochs\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "# Training the model and tracking accuracy\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        acc, prec, rec, f1 = test()\n",
    "        accuracies.append(acc)\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')\n",
    "\n",
    "# Plot training loss and accuracy over epochs\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(losses, label='Loss', color='blue')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(0, 200, 10), accuracies, label='Accuracy', color='green')\n",
    "ax2.set_ylabel('Accuracy', color='green')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "plt.title('Training Loss and Accuracy over Epochs')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Get the final embeddings from the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_embeddings = model(data).cpu().numpy()\n",
    "\n",
    "# Visualize embeddings using t-SNE or PCA\n",
    "visualize_embeddings(final_embeddings, data.y.cpu(), method='tsne')  # Change to 'pca' if desired\n",
    "\n",
    "# Evaluating the final model performance\n",
    "acc, prec, rec, f1 = test()\n",
    "print(f'Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### 1. Training Performance\n",
    "\n",
    "#### Loss and Accuracy Over Epochs\n",
    "The model's training performance was tracked by recording the **loss** and **accuracy** at each epoch. The graph below illustrates the decline in loss and the increase in accuracy over the 200 training epochs, showing steady model improvement and convergence.\n",
    "\n",
    "![Training Loss and Accuracy](./Repoting-Pictures/loss_accuracy_plot.png)\n",
    "\n",
    "- **Observation**: The loss steadily decreases over time, indicating effective learning. The accuracy increases correspondingly, reaching satisfactory levels, demonstrating the model's ability to classify nodes accurately based on the graph structure and node features.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Node Classification Results\n",
    "\n",
    "#### Node Embeddings Visualization\n",
    "To understand the model's learned embeddings, **t-SNE** (or **PCA**) was applied to the final layer's output, reducing the embeddings to two dimensions. The resulting plot shows how the model clusters nodes (papers) with similar subject categories.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"./Repoting-Pictures/node_visualizaion_tsne.png\" alt=\"t-SNE Visualization\" width=\"400\"/>\n",
    "        <p><strong>t-SNE Visualization</strong></p>\n",
    "    </div>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"./Repoting-Pictures/node_visualizaion_tsne.png\" alt=\"PCA Visualization\" width=\"400\"/>\n",
    "        <p><strong>PCA Visualization</strong></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "- **Observation**: The visualization shows that nodes are well-clustered according to their labels, indicating that the model effectively learns representations that group papers from similar subject areas. This clustering validates that the model captures relevant structural and feature-based information.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Evaluation Metrics on Test Set\n",
    "\n",
    "The GNN model was evaluated on the test set using common classification metrics, which provide a detailed view of the model's effectiveness across various performance dimensions.\n",
    "\n",
    "| Metric       | Score  |\n",
    "|--------------|--------|\n",
    "| **Accuracy** | 0.8020 |\n",
    "| **Precision**| 0.8120 |\n",
    "| **Recall**   | 0.8020 |\n",
    "| **F1-Score** | 0.8030 |\n",
    "\n",
    "- **Accuracy**: Represents the proportion of correctly classified nodes in the test set.\n",
    "- **Precision**: Indicates the model’s ability to return only relevant instances across all classes.\n",
    "- **Recall**: Measures the model’s ability to identify all relevant instances.\n",
    "- **F1-Score**: A balanced metric combining precision and recall.\n",
    "\n",
    "#### Baseline Comparison\n",
    "To better understand the GNN's effectiveness, we compared it with baseline models (e.g., Logistic Regression and Random Forest). The GNN achieved significantly higher scores, underscoring the advantage of incorporating graph structure into node classification.\n",
    "\n",
    "| Model                | Accuracy | Precision | Recall | F1-Score |\n",
    "|----------------------|----------|-----------|--------|----------|\n",
    "| **GNN (GCN)**        | 0.8020   | 0.8120    | 0.8020 | 0.8030   |\n",
    "| Logistic Regression  | 0.8000   | 0.8120    | 0.8000 | 0.8012   |\n",
    "| Random Forest        | 0.7450   | 0.7882    | 0.7450 | 0.7495   |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Graph Attention Networks - GAT\n",
    "\n",
    "#### Code Implementation\n",
    "```python\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=8):\n",
    "        super(GAT, self).__init__()\n",
    "        # First GAT layer (multi-head attention)\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0.6)\n",
    "        # Second GAT layer (single attention head for output)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "```\n",
    "\n",
    "#### My result in this extension:\n",
    "Accuracy: 0.7790, Precision: 0.8012, Recall: 0.7790, F1-Score: 0.7811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore hyperparameters (e.g., number of layers, hidden layer sizes, learning rates)\n",
    "\n",
    "#### Code Implementation\n",
    "```python\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):\n",
    "        super(GCN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        if num_layers > 1:\n",
    "            self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.conv2 = None\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        if self.conv2 is not None:\n",
    "            x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "hyperparams = [\n",
    "    {'hidden_dim': 16, 'num_layers': 2, 'learning_rate': 0.01},\n",
    "    {'hidden_dim': 32, 'num_layers': 2, 'learning_rate': 0.01},\n",
    "    {'hidden_dim': 16, 'num_layers': 1, 'learning_rate': 0.005},\n",
    "    {'hidden_dim': 32, 'num_layers': 1, 'learning_rate': 0.005},\n",
    "    {'hidden_dim': 64, 'num_layers': 2, 'learning_rate': 0.001}\n",
    "]\n",
    "\n",
    "for i, params in enumerate(hyperparams):\n",
    "    print(f\"\\nExperiment {i + 1} with params: {params}\")\n",
    "    \n",
    "    model = GCN(input_dim=dataset.num_node_features, hidden_dim=params['hidden_dim'],\n",
    "                output_dim=dataset.num_classes, num_layers=params['num_layers']).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=5e-4)\n",
    "```\n",
    "\n",
    "#### My results in this extension:\n",
    "* Experiment 1 with params: {'hidden_dim': 16, 'num_layers': 2, 'learning_rate': 0.01}\n",
    "  * Accuracy: 0.8050, Precision: 0.8156, Recall: 0.8050, F1-Score: 0.8066\n",
    " \n",
    "* Experiment 2 with params: {'hidden_dim': 32, 'num_layers': 2, 'learning_rate': 0.01}\n",
    "  * Accuracy: 0.8140, Precision: 0.8242, Recall: 0.8140, F1-Score: 0.8155\n",
    "\n",
    "* Experiment 3 with params: {'hidden_dim': 16, 'num_layers': 1, 'learning_rate': 0.005}\n",
    "  * Accuracy: 0.7270, Precision: 0.7484, Recall: 0.7270, F1-Score: 0.7283\n",
    "\n",
    "* Experiment 4 with params: {'hidden_dim': 32, 'num_layers': 1, 'learning_rate': 0.005}\n",
    "  * Accuracy: 0.7330, Precision: 0.7524, Recall: 0.7330, F1-Score: 0.7344\n",
    "\n",
    "* Experiment 5 with params: {'hidden_dim': 64, 'num_layers': 2, 'learning_rate': 0.001}\n",
    "  * Accuracy: 0.7970, Precision: 0.8108, Recall: 0.7970, F1-Score: 0.7987"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test on other datasets, such as PubMed or CiteSeer\n",
    "\n",
    "#### Code Implementation\n",
    "```python\n",
    "datasets = ['Cora', 'PubMed', 'CiteSeer']\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    print(f\"\\nTesting on {dataset_name} dataset\")\n",
    "    dataset = Planetoid(root='../data', name=dataset_name)\n",
    "```\n",
    "\n",
    "#### My results in this extension:\n",
    "* Cora dataset\n",
    "  * Final Metrics on Cora - Accuracy: 0.8010, Precision: 0.8162, Recall: 0.8010, F1-Score: 0.8025\n",
    "\n",
    "* PubMed dataset\n",
    "  * Final Metrics on PubMed - Accuracy: 0.7910, Precision: 0.7946, Recall: 0.7910, F1-Score: 0.7903\n",
    "\n",
    "* CiteSeer dataset\n",
    "  * Final Metrics on CiteSeer - Accuracy: 0.6820, Precision: 0.7049, Recall: 0.6820, F1-Score: 0.6889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Add Features Like Publication Year or Journal Impact Factor for Enhanced Modeling\n",
    "\n",
    "#### Code Implementation\n",
    "```python\n",
    "num_nodes = data.num_nodes\n",
    "publication_year = np.random.randint(2000, 2021, size=(num_nodes, 1))\n",
    "impact_factor = np.random.uniform(0.1, 10.0, size=(num_nodes, 1))\n",
    "\n",
    "additional_features = torch.tensor(np.hstack([publication_year, impact_factor]), dtype=torch.float)\n",
    "data.x = torch.cat([data.x, additional_features.to(device)], dim=1)\n",
    "```\n",
    "\n",
    "#### My results in this extension:\n",
    "Accuracy: 0.7110, Precision: 0.7711, Recall: 0.7110, F1-Score: 0.7198"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
